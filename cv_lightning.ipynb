{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850a397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from lightning import Trainer, LightningModule, LightningDataModule\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8535c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "num_epochs = 10\n",
    "noise_dim = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072eeb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.MNIST(root='./data', train=True, download=True)\n",
    "        datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.mnist_train = datasets.MNIST(root='./data', train=True, transform=self.transform)\n",
    "        self.mnist_test = datasets.MNIST(root='./data', train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbb76f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [06:22<00:00, 25.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 128kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:59<00:00, 27.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.03MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "module = MNISTDataModule(batch_size=batch_size)\n",
    "module.prepare_data()\n",
    "module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "733c38b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = module.train_dataloader()\n",
    "next(iter(dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a50fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_MNIST_Model(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            # Вход: вектор шума размера noise_dim\n",
    "            nn.Linear(noise_dim, 256 * 7 * 7),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (256, 7, 7)),\n",
    "            # Состояние: (256, 7, 7)\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 128, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            ),  # -> (128, 14, 14)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 1, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            ),  # -> (1, 28, 28)\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            # Вход: изображение (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                1, 64, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            ),  # -> (64, 14, 14)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(\n",
    "                64, 128, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            ),  # -> (128, 7, 7)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.real_label = 1.0\n",
    "        self.fake_label = 0.0\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx=None):\n",
    "        real_images = batch\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            #self.generator.zero_grad()\n",
    "            label = torch.full((batch_size,), self.real_label, device=self.device)\n",
    "            noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "            self.fake_images = self.generator(noise)\n",
    "            output = self.discriminator(self.fake_images).view(-1)\n",
    "            errG = self.criterion(output, label)\n",
    "            D_G_z2 = output.mean().item()\n",
    "            self.log('D_G_z2', D_G_z2, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "            #errG.backward()\n",
    "            #optimizer_g.step()\n",
    "            return errG\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            label = torch.full((batch_size,), self.real_label, device=self.device)\n",
    "            output = self.discriminator(real_images).view(-1)\n",
    "            errD_real = self.criterion(output, label)\n",
    "            #errD_real.backward()\n",
    "            \n",
    "            D_x = output.mean().item()\n",
    "            # Обучение генератора на фейковых изображениях\n",
    "        \n",
    "            #noise = torch.randn(batch_size, noise_dim, device=self.device)\n",
    "            #fake_images = self.generator(noise)    \n",
    "            label.fill_(self.fake_label)\n",
    "            #label = torch.full((batch_size,), self.fake_label, device=self.device)\n",
    "            output = self.discriminator(self.fake_images.detach()).view(-1)\n",
    "            errD_fake = self.criterion(output, label)\n",
    "            #errD_fake.backward()\n",
    "\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        self.log('errD', errD, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('D_x', D_x, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return errD_fake\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_g = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        optimizer_d = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        return [optimizer_g, optimizer_d], []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
